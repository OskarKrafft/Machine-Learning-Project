{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OskarKrafft/Machine-Learning-Project/blob/main/02_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the clean data**"
      ],
      "metadata": {
        "id": "_N-3JmRnXZ1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/Machine-Learning-Project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZJPM0y797iW",
        "outputId": "01e631c3-a839-4f12-9787-2871f76f14d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/Machine-Learning-Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "AWrMwwkD9mQR",
        "outputId": "b7b04007-5c58-47eb-b5c5-666c30dc5319"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       q1.1  q1.2  q1.3  q1.4  q1.5  q1.6  q1.7  q1.8  q1.9  q1.10  ...  d43a  \\\n",
              "0       1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
              "1       0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
              "2       1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
              "3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   1.0   \n",
              "4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0    0.0  ...   2.0   \n",
              "...     ...   ...   ...   ...   ...   ...   ...   ...   ...    ...  ...   ...   \n",
              "27459   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
              "27460   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
              "27461   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   1.0   \n",
              "27462   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
              "27463   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
              "\n",
              "       d43b  d46.8  d60  d62_1  d62_2  d63  d72_1  d72_2  d77  \n",
              "0       1.0    1.0  1.0    3.0    6.0  1.0    3.0    3.0  2.0  \n",
              "1       1.0    1.0  3.0    2.0    6.0  3.0    2.0    2.0  3.0  \n",
              "2       2.0    1.0  1.0    1.0    5.0  2.0    2.0    2.0  1.0  \n",
              "3       1.0    1.0  2.0    1.0    1.0  3.0    2.0    2.0  1.0  \n",
              "4       1.0    1.0  1.0    1.0    5.0  2.0    2.0    2.0  3.0  \n",
              "...     ...    ...  ...    ...    ...  ...    ...    ...  ...  \n",
              "27459   1.0    1.0  2.0    1.0    1.0  3.0    2.0    2.0  2.0  \n",
              "27460   1.0    1.0  2.0    1.0    1.0  3.0    2.0    2.0  2.0  \n",
              "27461   1.0    0.0  3.0    6.0    6.0  3.0    4.0    2.0  2.0  \n",
              "27462   1.0    1.0  2.0    6.0    6.0  4.0    2.0    2.0  2.0  \n",
              "27463   1.0    1.0  2.0    1.0    1.0  3.0    5.0    2.0  3.0  \n",
              "\n",
              "[27464 rows x 291 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-889f21b8-1d6e-474e-bf01-cf5213a650cb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q1.1</th>\n",
              "      <th>q1.2</th>\n",
              "      <th>q1.3</th>\n",
              "      <th>q1.4</th>\n",
              "      <th>q1.5</th>\n",
              "      <th>q1.6</th>\n",
              "      <th>q1.7</th>\n",
              "      <th>q1.8</th>\n",
              "      <th>q1.9</th>\n",
              "      <th>q1.10</th>\n",
              "      <th>...</th>\n",
              "      <th>d43a</th>\n",
              "      <th>d43b</th>\n",
              "      <th>d46.8</th>\n",
              "      <th>d60</th>\n",
              "      <th>d62_1</th>\n",
              "      <th>d62_2</th>\n",
              "      <th>d63</th>\n",
              "      <th>d72_1</th>\n",
              "      <th>d72_2</th>\n",
              "      <th>d77</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27459</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27460</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27461</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27462</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27463</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>27464 rows × 291 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-889f21b8-1d6e-474e-bf01-cf5213a650cb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-889f21b8-1d6e-474e-bf01-cf5213a650cb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-889f21b8-1d6e-474e-bf01-cf5213a650cb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# Importing the data\n",
        "\n",
        "import pandas as pd\n",
        "eppes_clean = pd.read_csv('eppes_clean_categorical.csv')\n",
        "eppes_clean = eppes_clean.drop(eppes_clean.columns[0], axis = 1)\n",
        "eppes_clean"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For test purposes: reduce to first 500 rows\n",
        "\n",
        "eppes_clean = eppes_clean[:5000]\n",
        "len(eppes_clean)\n",
        "\n",
        "# Examining data types \n",
        "eppes_clean.dtypes.value_counts() # only floats - fix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbUoAiPmGmj7",
        "outputId": "b27f82f3-d583-46d7-b3ff-03709a1bbed3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "float64    291\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining train and test data**"
      ],
      "metadata": {
        "id": "6Ww5qFbbX7s6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiCL2hxOv1fP",
        "outputId": "b298fbd0-4277-4a19-8465-2b6a9e5eb273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   q1.1  q1.2  q1.3  q1.4  q1.5  q1.6  q1.7  q1.8  q1.9  q1.10  ...  d43a  \\\n",
            "0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
            "1   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
            "2   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
            "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   1.0   \n",
            "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0    0.0  ...   2.0   \n",
            "\n",
            "   d43b  d46.8  d60  d62_1  d62_2  d63  d72_1  d72_2  d77  \n",
            "0   1.0    1.0  1.0    3.0    6.0  1.0    3.0    3.0  2.0  \n",
            "1   1.0    1.0  3.0    2.0    6.0  3.0    2.0    2.0  3.0  \n",
            "2   2.0    1.0  1.0    1.0    5.0  2.0    2.0    2.0  1.0  \n",
            "3   1.0    1.0  2.0    1.0    1.0  3.0    2.0    2.0  1.0  \n",
            "4   1.0    1.0  1.0    1.0    5.0  2.0    2.0    2.0  3.0  \n",
            "\n",
            "[5 rows x 291 columns]\n"
          ]
        }
      ],
      "source": [
        "# Define X and y\n",
        "\n",
        "print(eppes_clean.head())\n",
        "\n",
        "X = eppes_clean.drop(columns='qg1') # reference variable which contains voted y/n\n",
        "y = eppes_clean['qg1'] # reference variable which contains voted y/n\n",
        "\n",
        "# 80/20 train-test split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHBWYXv82Dt-",
        "outputId": "d121741b-8ce4-4cca-a453-9f991c990c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     q1.1  q1.2  q1.3  q1.4  q1.5  q1.6  q1.7  q1.8  q1.9  q1.10  ...  d43a  \\\n",
            "199   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   1.0   \n",
            "450   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
            "231   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
            "95    1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   1.0   \n",
            "54    1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
            "..    ...   ...   ...   ...   ...   ...   ...   ...   ...    ...  ...   ...   \n",
            "98    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    1.0  ...   2.0   \n",
            "476   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
            "322   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   1.0   \n",
            "382   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   2.0   \n",
            "365   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0  ...   1.0   \n",
            "\n",
            "     d43b  d46.8  d60  d62_1  d62_2  d63  d72_1  d72_2  d77  \n",
            "199   1.0    0.0  3.0    6.0    6.0  2.0    3.0    3.0  4.0  \n",
            "450   1.0    1.0  3.0    1.0    6.0  3.0    2.0    2.0  4.0  \n",
            "231   1.0    1.0  2.0    1.0    1.0  3.0    1.0    1.0  2.0  \n",
            "95    1.0    1.0  3.0    1.0    6.0  1.0    4.0    4.0  2.0  \n",
            "54    1.0    1.0  3.0    1.0    1.0  4.0    3.0    3.0  3.0  \n",
            "..    ...    ...  ...    ...    ...  ...    ...    ...  ...  \n",
            "98    1.0    1.0  3.0    1.0    1.0  3.0    3.0    2.0  2.0  \n",
            "476   1.0    0.0  2.0    1.0    6.0  2.0    4.0    2.0  3.0  \n",
            "322   1.0    1.0  3.0    1.0    2.0  3.0    1.0    1.0  3.0  \n",
            "382   1.0    0.0  3.0    6.0    6.0  3.0    2.0    2.0  3.0  \n",
            "365   1.0    1.0  3.0    1.0    1.0  3.0    2.0    2.0  3.0  \n",
            "\n",
            "[400 rows x 290 columns]\n"
          ]
        }
      ],
      "source": [
        "X_train\n",
        "y_train\n",
        "\n",
        "y_train.value_counts(normalize=True)\n",
        "print(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining pre-processing steps** \n",
        "\n",
        "All categorical variables are OneHotEncoded. Age is the only truly continuous variable in our dataset, which is already normally distributed and positive. Thus, we do not employ any transformation of the numerical variables. "
      ],
      "metadata": {
        "id": "aqxO1tsKZOfy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "cATESRMVQDZ9"
      },
      "outputs": [],
      "source": [
        "# Setting up pre-processing pipeline\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Identify all categorical variables by data type\n",
        "categorical_X_features = X_train.select_dtypes(include=['object', 'bool']).columns\n",
        "\n",
        "# OneHotEncode all categorical variables\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"error\")\n",
        "\n",
        "preprocessor = ColumnTransformer(remainder = 'passthrough', # remainder = passthrough for numerical variables to be kept unchanged\n",
        "    transformers=[\n",
        "        (\"cat\", categorical_transformer, categorical_X_features)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "_eJ6L3I29mQX",
        "outputId": "e3fa3749-37a0-475c-8b95-852e473bddcd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      0    1    2    3    4    5    6    7    8    9    ...  280  281  282  \\\n",
              "0     0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0  1.0   \n",
              "1     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  2.0  1.0  1.0   \n",
              "2     0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0  1.0   \n",
              "3     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0  1.0   \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  2.0  1.0  1.0   \n",
              "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "3995  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  2.0  1.0  0.0   \n",
              "3996  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0  1.0   \n",
              "3997  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  2.0  1.0  1.0   \n",
              "3998  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0  0.0   \n",
              "3999  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  2.0  1.0  1.0   \n",
              "\n",
              "      283  284  285  286  287  288  289  \n",
              "0     3.0  3.0  7.0  1.0  2.0  1.0  3.0  \n",
              "1     2.0  1.0  1.0  3.0  2.0  2.0  2.0  \n",
              "2     3.0  1.0  7.0  2.0  2.0  2.0  2.0  \n",
              "3     3.0  1.0  1.0  3.0  2.0  2.0  3.0  \n",
              "4     3.0  1.0  6.0  3.0  1.0  1.0  1.0  \n",
              "...   ...  ...  ...  ...  ...  ...  ...  \n",
              "3995  3.0  7.0  7.0  3.0  2.0  1.0  3.0  \n",
              "3996  2.0  2.0  6.0  2.0  4.0  4.0  4.0  \n",
              "3997  3.0  1.0  1.0  3.0  2.0  1.0  2.0  \n",
              "3998  2.0  1.0  6.0  3.0  2.0  2.0  2.0  \n",
              "3999  3.0  1.0  6.0  3.0  2.0  2.0  2.0  \n",
              "\n",
              "[4000 rows x 290 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f77e34c3-1b11-4a3b-8729-5850b81db265\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3998</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4000 rows × 290 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f77e34c3-1b11-4a3b-8729-5850b81db265')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f77e34c3-1b11-4a3b-8729-5850b81db265 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f77e34c3-1b11-4a3b-8729-5850b81db265');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# Inspect the number of variables after pre-processing\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "preprocessor.fit(X_train)\n",
        "X_train_ = preprocessor.transform(X_train)\n",
        "\n",
        "X_train_df = pd.DataFrame(data=X_train_)\n",
        "X_train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBFgZlBkyl2p"
      },
      "source": [
        "**Model 1 (Baseline): Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "C0154btCRIEf"
      },
      "outputs": [],
      "source": [
        "# Define a logistic regression model\n",
        "\n",
        "logistic_regression_pipe = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrMb4C8TJ-t-",
        "outputId": "d814d6b8-2f81-4f9f-d3d3-91e1453d2a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.903486 using {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
            "0.890228 (0.010019) with: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cg'}\n",
            "0.889553 (0.009687) with: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n",
            "0.889569 (0.009828) with: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
            "0.903259 (0.009322) with: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cg'}\n",
            "0.903119 (0.007908) with: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n",
            "0.903486 (0.008303) with: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
            "0.901193 (0.008902) with: {'classifier__C': 1.0, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cg'}\n",
            "0.903351 (0.007684) with: {'classifier__C': 1.0, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n",
            "0.901112 (0.009454) with: {'classifier__C': 1.0, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
            "0.900308 (0.010007) with: {'classifier__C': 10, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cg'}\n",
            "0.902802 (0.007696) with: {'classifier__C': 10, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n",
            "0.900176 (0.010406) with: {'classifier__C': 10, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
            "0.899768 (0.010249) with: {'classifier__C': 100, 'classifier__penalty': 'l2', 'classifier__solver': 'newton-cg'}\n",
            "0.903072 (0.007791) with: {'classifier__C': 100, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n",
            "0.899548 (0.010139) with: {'classifier__C': 100, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n"
          ]
        }
      ],
      "source": [
        "# Tune the hyperparameters\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# define parameters to be optimised, based on handbook and different online articles\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear'] # algorithms used to solve the optimization problem\n",
        "penalty = ['l2'] # specifying penaltty - limited to l2 as other penalties not compatible with all solvers\n",
        "c_values = [0.01, 0.1, 1.0, 10, 100] # inverse of regularization strength (smaller values = stronger regularization)\n",
        "\n",
        "# fit model\n",
        "param_grid = {\n",
        "    'classifier__solver':solvers, \n",
        "    'classifier__penalty':penalty,\n",
        "    'classifier__C':c_values}\n",
        "\n",
        "# Set-up repeated stratified cross-validation \n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=123)\n",
        "\n",
        "# Define GridSearchCV with F1 as comparison metrics\n",
        "grid_search = GridSearchCV(estimator=logistic_regression_pipe, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='f1')\n",
        "\n",
        "# Fit the grid search model\n",
        "grid_result = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# print the mean test scrore (currently accuracy), sd and the params that were used\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# document best params as vector\n",
        "from joblib import dump, load\n",
        "\n",
        "estimator = grid_result.best_estimator_\n",
        "dump(estimator, \"best-logistic-regression.joblib\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_0qmTlj4jo3",
        "outputId": "dc1785c8-d973-4e68-97c0-bb938ee2faad"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['best-logistic-regression.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get classification report on best performing model\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "best_logistic_regression = load(\"best-logistic-regression.joblib\")\n",
        "best_logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "y_pred_logistic = best_logistic_regression.predict(X_train)\n",
        "\n",
        "print(y_pred_logistic)\n",
        "\n",
        "print(classification_report(y_train, y_pred_logistic)) \n",
        "\n",
        "visualizer = ClassificationReport(best_logistic_regression, classes=[1.0, 2.0])\n",
        "visualizer.fit(X_train, y_train)  \n",
        "visualizer.score(X_train, y_train)\n",
        "visualizer.poof()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW3Rp9uRJvQ1",
        "outputId": "ee3e2ebb-ce53-4362-cd42-3ecbe76d8fd9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. ... 1. 2. 2.]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.89      0.94      0.92      2672\n",
            "         2.0       0.87      0.77      0.82      1328\n",
            "\n",
            "    accuracy                           0.88      4000\n",
            "   macro avg       0.88      0.86      0.87      4000\n",
            "weighted avg       0.88      0.88      0.88      4000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfqsH6Vuyter"
      },
      "source": [
        "**Model 2: Naive Bayesian**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "f1EYEUHUdMky"
      },
      "outputs": [],
      "source": [
        "### Define transformers\n",
        "\n",
        "# numeric_transformer = MinMaxScaler() # no transformation for numeric data\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "\n",
        "preprocessor = ColumnTransformer(remainder = 'passthrough', # passthrough so numeric columns are not dropped\n",
        "    transformers=[(\"cat\", categorical_transformer, categorical_X_features)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CategoricalNB seems most suitable as it assumes categorical distribution (there seems not explicit solution for ordinal features). \n",
        "# Alternatives are less relevant, e.g. Gaussian NB is for continuous and Bernoulli for binary\n",
        "\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "\n",
        "### Set up the pipeline\n",
        "naive_bayes_pipe = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", CategoricalNB())]\n",
        ")"
      ],
      "metadata": {
        "id": "EWOuMM6tgyFi"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DDTL0SMmJ7C9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "RDRSUhQT2B_U"
      },
      "outputs": [],
      "source": [
        "### Fit and predict the model\n",
        "\n",
        "naive_bayes_pipe.fit(X_train, y_train)\n",
        "y_pred_bayes = naive_bayes_pipe.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Assess the model performance\n",
        "\n",
        "# print classificarion statistics\n",
        "print(classification_report(y_train, y_pred_bayes)) \n",
        "\n",
        "### Examine frequencies in y (predicted and actual training y)\n",
        "y_pred_df = pd.DataFrame(y_pred_bayes)\n",
        "print(\"Frequency in y train:\", \"\\n\", y_train.value_counts(normalize=True), \"\\n\")\n",
        "print(\"Frequency in naive bayes prediction:\", \"\\n\",y_pred_df.value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x54VkAJNgtk",
        "outputId": "772d8fb4-041e-4bbd-eb95-23a45caa9ddd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.81      0.76      0.79      2672\n",
            "         2.0       0.57      0.64      0.61      1328\n",
            "\n",
            "    accuracy                           0.72      4000\n",
            "   macro avg       0.69      0.70      0.70      4000\n",
            "weighted avg       0.73      0.72      0.73      4000\n",
            "\n",
            "Frequency in y train: \n",
            " 1.0    0.668\n",
            "2.0    0.332\n",
            "Name: qg1, dtype: float64 \n",
            "\n",
            "Frequency in naive bayes prediction: \n",
            " 1.0    0.6265\n",
            "2.0    0.3735\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ueeuzhayHzP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 3: SVM**"
      ],
      "metadata": {
        "id": "NQ-Cg-wUtUpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Define transformers\n",
        "\n",
        "# numeric_transformer = MinMaxScaler() # no transformation for numeric data\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "\n",
        "preprocessor = ColumnTransformer(remainder = 'passthrough', # passthrough so numeric columns are not dropped\n",
        "    transformers=[(\"cat\", categorical_transformer, categorical_X_features)])"
      ],
      "metadata": {
        "id": "pF620mDNtiNL"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "### Set up the pipeline\n",
        "SVM_pipe = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", SVC())]\n",
        ")"
      ],
      "metadata": {
        "id": "UTp9ZecFtl2A"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cmjyVSR60SAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameters\n",
        "kernel = ['rbf'] # specifies the kernel type to be used in the algorithm\n",
        "c_values = [0.1, 1.0, 10, 100] # inverse of regularization strength (smaller values = stronger regularization)\n",
        "gamma_values = [1, 0.1, 0.01, 0.001]\n",
        "\n",
        "# set up param grid for GridSearch\n",
        "param_grid = {\n",
        "    'classifier__kernel':kernel, \n",
        "    'classifier__C':c_values,\n",
        "    'classifier__gamma':gamma_values}\n",
        "\n",
        "# set up GridSearch\n",
        "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=123)\n",
        "grid_search = GridSearchCV(estimator=SVM_pipe, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='f1')\n",
        "\n",
        "# fit the grid search model\n",
        "grid_result = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# print the mean test scrore (currently accuracy), sd and the params that were used\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ez88h0jt1VE",
        "outputId": "2e96fd75-2ecc-4a60-b9d6-d26424cf88aa"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.901682 using {'classifier__C': 10, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}\n",
            "0.800959 (0.000440) with: {'classifier__C': 0.1, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}\n",
            "0.800959 (0.000440) with: {'classifier__C': 0.1, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n",
            "0.800959 (0.000440) with: {'classifier__C': 0.1, 'classifier__gamma': 0.01, 'classifier__kernel': 'rbf'}\n",
            "0.828883 (0.004667) with: {'classifier__C': 0.1, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}\n",
            "0.800959 (0.000440) with: {'classifier__C': 1.0, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}\n",
            "0.800959 (0.000440) with: {'classifier__C': 1.0, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n",
            "0.846593 (0.006021) with: {'classifier__C': 1.0, 'classifier__gamma': 0.01, 'classifier__kernel': 'rbf'}\n",
            "0.858010 (0.005822) with: {'classifier__C': 1.0, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}\n",
            "0.800959 (0.000440) with: {'classifier__C': 10, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}\n",
            "0.800959 (0.000440) with: {'classifier__C': 10, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n",
            "0.849957 (0.005698) with: {'classifier__C': 10, 'classifier__gamma': 0.01, 'classifier__kernel': 'rbf'}\n",
            "0.901682 (0.006255) with: {'classifier__C': 10, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}\n",
            "0.800959 (0.000440) with: {'classifier__C': 100, 'classifier__gamma': 1, 'classifier__kernel': 'rbf'}\n",
            "0.800959 (0.000440) with: {'classifier__C': 100, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n",
            "0.849957 (0.005698) with: {'classifier__C': 100, 'classifier__gamma': 0.01, 'classifier__kernel': 'rbf'}\n",
            "0.872872 (0.009510) with: {'classifier__C': 100, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# document best params as vector\n",
        "estimator = grid_result.best_estimator_\n",
        "dump(estimator, \"best-SVM.joblib\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Oo7GLR54dB8",
        "outputId": "3509367c-5b22-4503-fdb2-605c93102743"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['best-SVM.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 4: Random Forest**"
      ],
      "metadata": {
        "id": "mFQHH-YzHy8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get classification report on best performing model\n",
        "best_SVM = load(\"best-SVM.joblib\")\n",
        "best_SVM.fit(X_train, y_train)\n",
        "\n",
        "y_pred_SVM = best_SVM.predict(X_train)\n",
        "\n",
        "print(y_pred_SVM)\n",
        "print(classification_report(y_train, y_pred_SVM)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYRm0D_W4qSo",
        "outputId": "7cc7974e-c897-4469-e611-c380a6e7b997"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 2. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.\n",
            " 1. 2. 1. 2. 2. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 2. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2.\n",
            " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.94      1.00      0.97       332\n",
            "         2.0       1.00      0.69      0.82        68\n",
            "\n",
            "    accuracy                           0.95       400\n",
            "   macro avg       0.97      0.85      0.89       400\n",
            "weighted avg       0.95      0.95      0.94       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Define transformers\n",
        "\n",
        "# numeric_transformer = MinMaxScaler() # no transformation for numeric data\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "\n",
        "preprocessor = ColumnTransformer(remainder = 'passthrough', # passthrough so numeric columns are not dropped\n",
        "    transformers=[(\"cat\", categorical_transformer, categorical_X_features)])"
      ],
      "metadata": {
        "id": "D18GE5bW5z92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hY5rm_j8BM5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "### Set up the pipeline\n",
        "random_forest_pipe = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier(random_state = 123))])"
      ],
      "metadata": {
        "id": "XcbJwdgP-pCG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameters\n",
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)] # number of trees in the random forest\n",
        "max_features = ['auto', 'sqrt'] # number of features in consideration at every split\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)] # maximum number of levels allowed in each decision tree\n",
        "min_samples_split = [2, 5, 10] # minimum sample number to split a node\n",
        "min_samples_leaf = [1, 2, 4] # minimum sample number that can be stored in a leaf node\n",
        "\n",
        "# set up param grid for GridSearch\n",
        "random_grid = {\n",
        "    'classifier__n_estimators':n_estimators, \n",
        "    'classifier__max_features':max_features,\n",
        "    'classifier__max_depth':max_depth,\n",
        "    'classifier__min_samples_split':min_samples_split,\n",
        "    'classifier__min_samples_leaf': min_samples_leaf}\n",
        "\n",
        "# set up GridSearch\n",
        "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=123)\n",
        "RandomizedSearchCV(estimator = random_forest_pipe , param_distributions = random_grid, cv = cv, random_state=123, n_jobs = -1)\n",
        "\n",
        "# fit the grid search model\n",
        "grid_result = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# print the mean test scrore (currently accuracy), sd and the params that were used\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "id": "jaoHhXuW-ppj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Comparing the best performing models\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "\n",
        "models = []\n",
        "models.append(('Logistic Regression', load(\"best-logistic-regression.joblib\")))\n",
        "models.append(('Naive Bayes', naive_bayes_pipe))\n",
        "models.append(('SVM', load(\"best-SVM.joblib\")))\n",
        "\n",
        "print(models)\n",
        "\n",
        "y_train.value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "H2D1hzFRJbAo",
        "outputId": "15d8e94c-8df5-4a40-bb3d-c1b5dc0b52f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Logistic Regression', Pipeline(steps=[('preprocessor',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('cat', OneHotEncoder(),\n",
            "                                                  Index([], dtype='object'))])),\n",
            "                ('classifier', LogisticRegression(C=0.1, solver='newton-cg'))])), ('Naive Bayes', Pipeline(steps=[('preprocessor',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('cat',\n",
            "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
            "                                                  Index([], dtype='object'))])),\n",
            "                ('classifier', CategoricalNB())])), ('SVM', Pipeline(steps=[('preprocessor',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('cat',\n",
            "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
            "                                                  Index([], dtype='object'))])),\n",
            "                ('classifier', SVC(gamma=0.001, kernel='poly'))]))]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    0.83\n",
              "2.0    0.17\n",
              "Name: qg1, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.classifier import ClassificationReport\n",
        "\n",
        "for name, model in models:\n",
        "  visualizer = ClassificationReport(model, classes=[1.0, 2.0])\n",
        "  visualizer.fit(X_train, y_train)  \n",
        "  visualizer.score(X_train, y_train)\n",
        "  visualizer.poof()\n",
        "\n",
        "\n",
        "y_train.value_counts"
      ],
      "metadata": {
        "id": "QBrP9MyIKkIf",
        "outputId": "6f6bcd00-99eb-4d6a-933c-dc66061aecd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method IndexOpsMixin.value_counts of 199    1.0\n",
              "450    1.0\n",
              "231    1.0\n",
              "95     1.0\n",
              "54     2.0\n",
              "      ... \n",
              "98     2.0\n",
              "476    1.0\n",
              "322    2.0\n",
              "382    2.0\n",
              "365    1.0\n",
              "Name: qg1, Length: 400, dtype: float64>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "\n",
        "for name, model in models:\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=123)\n",
        "  cv_results = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "\n",
        "\n",
        "print(cv_results.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyliL1IDHbrr",
        "outputId": "2819de48-4956-4a58-bee9-5fcc3a0523f7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 5 is out of bounds for axis 1 with size 5\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 4 is out of bounds for axis 1 with size 4\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 5 is out of bounds for axis 1 with size 5\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 6 is out of bounds for axis 1 with size 6\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 7 is out of bounds for axis 1 with size 4\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 4 is out of bounds for axis 1 with size 4\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 3 is out of bounds for axis 1 with size 3\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 7 is out of bounds for axis 1 with size 4\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 94 is out of bounds for axis 1 with size 93\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 9 is out of bounds for axis 1 with size 9\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 7 is out of bounds for axis 1 with size 4\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 4 is out of bounds for axis 1 with size 4\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 6 is out of bounds for axis 1 with size 6\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 3 is out of bounds for axis 1 with size 3\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
            "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 113, in <lambda>\n",
            "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 470, in predict\n",
            "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 83, in predict\n",
            "    jll = self._joint_log_likelihood(X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 1461, in _joint_log_likelihood\n",
            "    jll += self.feature_log_prob_[i][:, indices].T\n",
            "IndexError: index 1 is out of bounds for axis 1 with size 1\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8699999999999999\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.5 ('Machine-Learning-Project-K3kAtfCK')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d776d6ed3af7d3b49847a0099a298aa8d78d20c8f42c0601505d3117fd4dfc67"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}